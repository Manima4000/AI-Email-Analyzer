# Classificador de E-mails com IA üìß‚ú®

![Badge de Status](https://img.shields.io/badge/status-funcional-brightgreen)
![Badge do Python](https://img.shields.io/badge/Python-3.11+-blue.svg)
![Badge do FastAPI](https://img.shields.io/badge/FastAPI-0.100+-green.svg)
![Badge do Frontend](https://img.shields.io/badge/Frontend-Vanilla%20JS-orange)

![GIF da Aplica√ß√£o em Funcionamento](https://via.placeholder.com/800x400.png?text=Insira+um+GIF+da+sua+aplica√ß√£o+aqui)

> Um classificador de e-mails inteligente que utiliza a API do Google Gemini para categorizar mensagens como "Produtivas" ou "Improdutivas" e sugere respostas autom√°ticas, tudo atrav√©s de uma interface web limpa e moderna.

**üîó Link para a Demo Ao Vivo:** `[EM BREVE]`

---

## üìö Tabela de Conte√∫dos

* [Sobre o Projeto](#-sobre-o-projeto)
* [Tecnologias Utilizadas](#-tecnologias-utilizadas)
* [Principais Features](#-principais-features)
* [Como Rodar o Projeto Localmente](#-como-rodar-o-projeto-localmente)
  * [Pr√©-requisitos](#pr√©-requisitos)
  * [M√©todo 1: Docker (Recomendado)](#m√©todo-1-docker-recomendado)
  * [M√©todo 2: Manualmente](#m√©todo-2-manualmente)
* [Escolhendo o Modelo de IA](#-escolhendo-o-modelo-de-ia-opcional)
* [Rodando os Testes](#-rodando-os-testes)
* [Estrat√©gia de Deploy](#-estrat√©gia-de-deploy)
* [Pr√≥ximos Passos](#-pr√≥ximos-passos)
* [Contato](#-contato)

---

## üéØ Sobre o Projeto

Este projeto foi desenvolvido para simular uma ferramenta de automa√ß√£o e produtividade. A aplica√ß√£o permite que um usu√°rio cole o texto de um e-mail ou fa√ßa o upload de um arquivo (`.txt`/`.pdf`) e, em segundos, receba uma an√°lise gerada por IA sobre o conte√∫do, al√©m de uma sugest√£o de resposta pronta para uso.

O objetivo era construir uma aplica√ß√£o full-stack completa, demonstrando boas pr√°ticas de desenvolvimento, como:
* Arquitetura de microsservi√ßos desacoplada (frontend e backend).
* Testes automatizados para garantir a qualidade do c√≥digo.
* Configura√ß√£o flex√≠vel atrav√©s de vari√°veis de ambiente.
* Desenvolvimento orientado a uma experi√™ncia de usu√°rio limpa e moderna.

---

## üõ†Ô∏è Tecnologias Utilizadas

* **Backend:** Python, FastAPI, Uvicorn
* **Intelig√™ncia Artificial:** Google Gemini Pro API
* **Frontend:** HTML5, CSS3, JavaScript (Vanilla JS)
* **Testes:** Pytest, pytest-asyncio
* **Containeriza√ß√£o:** Docker, Docker Compose
* **Deploy:** Vercel (Frontend), Render (Backend)

---

## ‚ú® Principais Features

* **An√°lise por IA:** Classifica√ß√£o de e-mails e sugest√£o de respostas via Google Gemini.
* **M√∫ltiplos Formatos de Entrada:** Suporte para texto colado, upload de arquivos `.txt` e `.pdf`.
* **Interface Reativa:** Feedback visual de "carregando" durante o processamento.
* **Tema Claro e Escuro (Dark Mode):** Com persist√™ncia da escolha do usu√°rio.
* **Backend Ass√≠ncrono:** Constru√≠do com FastAPI para alta performance.
* **Modelo de IA Configur√°vel:** Permite a troca f√°cil do modelo Gemini via vari√°veis de ambiente.
* **Cobertura de Testes:** Testes de unidade e integra√ß√£o para garantir a robustez da API.

---

## üöÄ Como Rodar o Projeto Localmente

Siga as instru√ß√µes abaixo para ter o projeto rodando na sua m√°quina.

### Pr√©-requisitos
* [Git](https://git-scm.com/)
* [Python 3.11+](https://www.python.org/)
* [Docker](https://www.docker.com/products/docker-desktop/) e [Docker Compose](https://docs.docker.com/compose/install/) (para o m√©todo recomendado)

### M√©todo 1: Docker (Recomendado)

Esta √© a forma mais simples e r√°pida. O Docker cuida de toda a configura√ß√£o para voc√™.

1.  **Clone o reposit√≥rio:**
    ```bash
    git clone [https://github.com/seu-usuario/seu-repositorio.git](https://github.com/seu-usuario/seu-repositorio.git)
    cd seu-repositorio
    ```

2.  **Configure suas vari√°veis de ambiente:** * V√° para a pasta `backend`.
    * Crie uma c√≥pia do arquivo `.env.example` e renomeie-a para `.env`.
    * Abra o arquivo `.env` e preencha as vari√°veis:
        ```env
        # Cole aqui a chave que voc√™ obteve no Google AI Studio
        GOOGLE_API_KEY="SUA_CHAVE_API_AQUI"

        # (Opcional) Nome do modelo a ser usado. 'gemini-pro' √© um padr√£o seguro.
        MODEL_NAME="gemini-pro"
        ```

3.  **Suba os containers com Docker Compose:**
    * Na pasta raiz do projeto, execute:
    ```bash
    docker-compose up --build
    ```
    * O Docker ir√° construir a imagem do backend e iniciar o servidor.

4.  **Acesse a aplica√ß√£o:**
    * **Frontend:** Abra seu navegador e acesse `http://localhost:5500` (ou abra o arquivo `frontend/index.html` diretamente).
    * **Backend API Docs:** A documenta√ß√£o interativa da API estar√° dispon√≠vel em `http://localhost:8000/docs`.

### M√©todo 2: Manualmente

1.  **Clone o reposit√≥rio e navegue at√© ele.**

2.  **Configure e rode o Backend:**
    ```bash
    cd backend

    # Crie e ative o ambiente virtual
    python -m venv .venv
    # Windows
    .\.venv\Scripts\activate
    # macOS/Linux
    source .venv/bin/activate

    # O arquivo requirements.txt lista todas as bibliotecas Python que o projeto precisa.
    # Instale todas de uma vez com o comando abaixo:
    pip install -r requirements.txt # <-- ATUALIZADO

    # Crie o arquivo .env e adicione suas chaves (siga o passo 2 do m√©todo Docker)

    # Inicie o servidor
    uvicorn app.main:app --reload
    ```
    * O backend estar√° rodando em `http://localhost:8000`.

3.  **Rode o Frontend:**
    * A maneira mais f√°cil √© usar uma extens√£o como o **"Live Server"** no VS Code. Clique com o bot√£o direito no arquivo `frontend/index.html` e selecione "Open with Live Server".
    * Alternativamente, abra o arquivo `frontend/index.html` diretamente no seu navegador.

---

## ‚öôÔ∏è Escolhendo o Modelo de IA (Opcional)

A disponibilidade dos modelos do Gemini pode variar por conta ou regi√£o. Inclu√≠mos um script para voc√™ verificar quais modelos sua chave de API pode acessar e escolher o melhor.

1.  **Siga os passos do "M√©todo 2: Manualmente"** para configurar o ambiente do backend (ativar o `.venv` e instalar as depend√™ncias com `pip install -r requirements.txt`).

2.  **Rode o script de verifica√ß√£o:**
    * Na pasta `backend`, execute o comando:
    ```bash
    python check_models.py
    ```

3.  **Veja a sa√≠da:** O terminal listar√° os modelos dispon√≠veis, algo como:
    ```
    Listando modelos dispon√≠veis que suportam 'generateContent':
    models/gemini-pro
    models/gemini-1.0-pro
    models/gemini-1.5-flash-latest
    Script conclu√≠do.
    ```

4.  **Atualize seu arquivo `.env`:** Copie o nome de um dos modelos da lista (apenas a parte final, ex: `gemini-pro`) e cole-o como o valor da vari√°vel `MODEL_NAME` no seu arquivo `.env`.

---

## ‚úÖ Rodando os Testes

Para garantir que a l√≥gica do backend est√° funcionando corretamente, voc√™ pode rodar a su√≠te de testes automatizados.

1.  Navegue at√© a pasta `backend` e ative o ambiente virtual.
2.  Execute o `pytest`:
    ```bash
    # A partir da pasta 'backend'
    pytest
    ```

---

## ‚òÅÔ∏è Estrat√©gia de Deploy

A aplica√ß√£o √© projetada para um deploy moderno e desacoplado:

* **Frontend (Vercel):** O conte√∫do da pasta `frontend` pode ser implantado como um "Static Site" na Vercel. Basta conectar seu reposit√≥rio GitHub e a Vercel cuidar√° do resto.

* **Backend (Render):** O backend na pasta `backend` √© containerizado com Docker. A melhor abordagem √© implant√°-lo como um "Web Service" na plataforma Render, que se integra perfeitamente com o Docker. A Render fornecer√° uma URL p√∫blica para a sua API, que voc√™ dever√° configurar no seu `script.js` antes de fazer o deploy do frontend.

---

## üß≠ Pr√≥ximos Passos

Recursos planejados para futuras vers√µes:
* [ ] Streaming da resposta da IA (efeito "ChatGPT").
* [ ] Hist√≥rico de an√°lises salvo no navegador.
* [ ] Op√ß√£o de ajustar o "tom" da resposta sugerida (Formal, Amig√°vel, etc.).
* [ ] CI/CD com GitHub Actions para rodar os testes automaticamente.

---

## üë§ Contato

**[Seu Nome]**

* **LinkedIn:** `[Link para o seu LinkedIn]`
* **GitHub:** `[Link para o seu GitHub]`
* **E-mail:** `[Seu E-mail]`
