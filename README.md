# Classificador de E-mails com IA 

![Badge de Status](https://img.shields.io/badge/status-funcional-brightgreen)
![Badge do Python](https://img.shields.io/badge/Python-3.11+-blue.svg)
![Badge do FastAPI](https://img.shields.io/badge/FastAPI-0.100+-green.svg)
![Badge do Docker](https://img.shields.io/badge/Docker-conteinerizado-blue)


> Um classificador de e-mails inteligente que utiliza a API do Google Gemini para categorizar mensagens e sugerir respostas autom√°ticas. O projeto √© totalmente containerizado com Docker e possui uma interface web limpa e moderna.


**üîó Link para a Demo Ao Vivo:** `https://ai-email-frontend-nine.vercel.app/`

---

## Tabela de Conte√∫dos

* [Sobre o Projeto](#-sobre-o-projeto)
* [Tecnologias Utilizadas](#-tecnologias-utilizadas)
* [Principais Features](#-principais-features)
* [Como Rodar o Projeto Localmente](#-como-rodar-o-projeto-localmente)
  * [Pr√©-requisitos](#pr√©-requisitos)
  * [Instru√ß√µes (Docker)](#instru√ß√µes-docker)
* [Rodando os Testes](#-rodando-os-testes)
* [Configura√ß√£o Avan√ßada](#-configura√ß√£o-avan√ßada)
  * [Escolhendo o Modelo de IA](#escolhendo-o-modelo-de-ia)
  * [Rodando Manualmente (Sem Docker)](#rodando-manualmente-sem-docker)
* [Estrat√©gia de Deploy](#-estrat√©gia-de-deploy)


---

## Sobre o Projeto

Este projeto simula uma ferramenta de automa√ß√£o e produtividade. A aplica√ß√£o permite que um usu√°rio cole o texto de um e-mail ou fa√ßa o upload de um arquivo (`.txt`/`.pdf`) e, em segundos, receba uma an√°lise gerada por IA sobre o conte√∫do, al√©m de uma sugest√£o de resposta.

O objetivo era construir uma aplica√ß√£o full-stack completa, demonstrando boas pr√°ticas de desenvolvimento, como:
* **Containeriza√ß√£o:** Uso de Docker e Docker Compose para garantir portabilidade e facilidade de execu√ß√£o.
* **Arquitetura Desacoplada:** Frontend e backend como servi√ßos independentes.
* **Qualidade de C√≥digo:** Cobertura de testes automatizados com Pytest.
* **Configura√ß√£o Flex√≠vel:** Uso de vari√°veis de ambiente (`.env`) para gerenciar chaves e configura√ß√µes.

---

## Tecnologias Utilizadas

* **Backend:** Python, FastAPI, Uvicorn, BeautifulSoup4
* **Intelig√™ncia Artificial:** Google Gemini Pro API
* **Frontend:** HTML5, CSS3, JavaScript (Vanilla JS), NGINX
* **Testes:** Pytest, pytest-asyncio
* **Containeriza√ß√£o:** Docker, Docker Compose
* **Deploy:** Vercel (Frontend), Render (Backend)

---

## Principais Features

* **An√°lise por IA:** Sugest√£o de respostas via Google Gemini.
* **M√∫ltiplos Formatos de Entrada:** Suporte para texto colado e upload de arquivos `.txt` e `.pdf`.
* **Pr√©-processamento Inteligente de Texto:** Limpeza autom√°tica de tags HTML e respostas de e-mails antigos.
* **Interface Reativa:** Feedback visual de "carregando" durante o processamento.
* **Tema Claro e Escuro (Dark Mode):** Com persist√™ncia da escolha do usu√°rio.
* **Modelo de IA Configur√°vel:** Permite a troca f√°cil do modelo Gemini.
* **Execu√ß√£o Simplificada:** O projeto inteiro (frontend e backend) sobe com um √∫nico comando `docker-compose up`.
* **Cobertura de Testes:** Testes de unidade e integra√ß√£o para garantir a robustez da API.

---

## üöÄ Como Rodar o Projeto Localmente

Siga os passos abaixo para configurar e executar a aplica√ß√£o completa em seu ambiente de desenvolvimento local.

### Pr√©-requisitos
* [Git](https://git-scm.com/)
* [Python 3.11+](https://www.python.org/)
* [Docker](https://www.docker.com/products/docker-desktop/) e [Docker Compose](https://docs.docker.com/compose/install/)

### Passo 1: Clone o Reposit√≥rio
```bash
git clone [https://github.com/Manima4000/AI-Email-Analyzer.git](https://github.com/Manima4000/AI-Email-Analyzer.git)
cd AI-Email-Analyzer
```

### Passo 2: Configure o Backend
Crie o arquivo de vari√°veis de ambiente para o backend.

* Na pasta `backend`, crie uma c√≥pia do arquivo `.env.example` e renomeie-a para `.env`.
* Abra o arquivo `.env` e preencha as vari√°veis:
    ```env
    GOOGLE_API_KEY="SUA_CHAVE_API_AQUI"
    MODEL_NAME="gemini-pro"
    ```

### Passo 3: Configure o Frontend para o Ambiente Local
O c√≥digo no reposit√≥rio est√° configurado para usar a API em produ√ß√£o. Para desenvolvimento local, voc√™ precisa apontar o frontend para o seu backend local.

* Abra o arquivo `frontend/script.js`.
* Encontre a linha que define a URL da API.
* Comente a linha da URL de produ√ß√£o e descomente a linha da URL local:
    ```javascript
    // const apiUrl = '[https://ai-email-backend-awbw.onrender.com//api/v1/classify-email](https://ai-email-backend-awbw.onrender.com//api/v1/classify-email)'; // <-- URL de Produ√ß√£o
    const apiUrl = '[http://127.0.0.1:8000/api/v1/classify-email](http://127.0.0.1:8000/api/v1/classify-email)'; // <-- URL Local para Desenvolvimento
    ```

### Passo 4: Execute a Aplica√ß√£o (Docker ou Manual)

#### M√©todo A: Docker (Recomendado)
Com o Docker, voc√™ sobe o frontend e o backend com um √∫nico comando.

1.  Verifique se o Docker Desktop est√° rodando.
2.  Na **pasta raiz do projeto**, execute:
    ```bash
    docker-compose up --build
    ```
3.  Acesse a aplica√ß√£o:
    * **Frontend:** **`http://localhost:5500`**
    * **Backend Docs:** **`http://localhost:8000/docs`**

#### M√©todo B: Manualmente
Execute o backend e o frontend em terminais separados.

1.  **Terminal 1 (Backend):**
    ```bash
    cd backend
    python -m venv .venv
    # Windows: .\.venv\Scripts\activate | macOS/Linux: source .venv/bin/activate
    pip install -r requirements.txt
    uvicorn app.main:app --reload
    ```

2.  **Terminal 2 (Frontend):**
    ```bash
    cd frontend
    python -m http.server 5500
    ```
3.  Acesse o frontend em **`http://localhost:5500`**.

---

## Rodando os Testes

Para garantir que a l√≥gica do backend est√° funcionando corretamente, voc√™ pode rodar a su√≠te de testes automatizados.

1.  **Siga os passos de configura√ß√£o do "M√©todo Manual"** (veja na se√ß√£o de Configura√ß√£o Avan√ßada) para criar e ativar o ambiente virtual (`.venv`) e instalar as depend√™ncias (`pip install -r requirements.txt`).

2.  **Execute o Pytest:**
    * Na pasta `backend`, execute o comando apropriado para o seu sistema operacional:

    * **No Windows (PowerShell):**
        ```powershell
        $env:PYTHONPATH="."; pytest -v
        ```

    * **No Windows (CMD):**
        ```powershell
        set PYTHONPATH=.
        pytest -v
        ```

    * **No macOS/Linux:**
        ```bash
        PYTHONPATH=. pytest -v
        ```

---

## Configura√ß√£o Avan√ßada

### Escolhendo o Modelo de IA
A disponibilidade dos modelos do Gemini pode variar. Inclu√≠mos um script para voc√™ verificar quais modelos sua chave de API pode acessar.

1.  Configure o ambiente do backend manualmente (veja o pr√≥ximo passo).
2.  Na pasta `backend`, execute o script de verifica√ß√£o:
    ```bash
    python check_models.py
    ```
3.  O terminal listar√° os modelos dispon√≠veis. Copie o nome de um deles e cole como o valor da vari√°vel `MODEL_NAME` no seu arquivo `.env`.

### Rodando Manualmente (Sem Docker)

1.  **Configure o Backend:**
    ```bash
    cd backend
    python -m venv .venv
    # Windows: .\.venv\Scripts\activate | macOS/Linux: source .venv/bin/activate

    # O arquivo requirements.txt lista todas as bibliotecas Python que o projeto precisa.
    pip install -r requirements.txt

    # Crie e configure o arquivo .env (como no m√©todo Docker)
    uvicorn app.main:app --reload
    ```
    * O backend estar√° rodando em `http://localhost:8000`.

2.  **Rode o Frontend:**
    * Use uma extens√£o como o **"Live Server"** no VS Code ou inicie um servidor Python na pasta `frontend`:
    ```bash
    cd frontend
    python -m http.server 5500
    ```
    * Acesse `http://localhost:5500`.

---

## Estrat√©gia de Deploy

* **Frontend (Vercel):** O conte√∫do da pasta `frontend` pode ser implantado como um "Static Site" na Vercel.
* **Backend (Render):** O backend na pasta `backend` √© containerizado e pode ser implantado como um "Web Service" na plataforma Render.

---

